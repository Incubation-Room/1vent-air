{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La reconnaissance d'objets par Yolo\n",
    "Exemple pris d'une photo de voie rapide avec reconnaissance de bus et voitures\n",
    "\n",
    "Le modèle utilisé est le yolov3\n",
    "Dans ce modèle les fichiers ci-dessous sont utilisés:\n",
    "- yolo/yolov3.weights\n",
    "- yolo/yolov3.cfg\n",
    "- yolo/coco.names\n",
    "\n",
    "Le 1er comporte l'information des Wi du réseau de neurone préentraîné avec 80 classes d'objets dont les bus, voitures. \n",
    "Le 2e représente le schéma du réseau de neurones et notamment les filtres de convolution construits.\n",
    "Le 3e labellise et nomme ces 80 classes préentraînées et reconnues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- len(layer_names) ---\n",
      "254\n",
      "--- type(layer_names) ---\n",
      "<class 'tuple'>\n",
      "--- layer_names ---\n",
      "('conv_0', 'bn_0', 'leaky_1', 'conv_1', 'bn_1', 'leaky_2', 'conv_2', 'bn_2', 'leaky_3', 'conv_3', 'bn_3', 'leaky_4', 'shortcut_4', 'conv_5', 'bn_5', 'leaky_6', 'conv_6', 'bn_6', 'leaky_7', 'conv_7', 'bn_7', 'leaky_8', 'shortcut_8', 'conv_9', 'bn_9', 'leaky_10', 'conv_10', 'bn_10', 'leaky_11', 'shortcut_11', 'conv_12', 'bn_12', 'leaky_13', 'conv_13', 'bn_13', 'leaky_14', 'conv_14', 'bn_14', 'leaky_15', 'shortcut_15', 'conv_16', 'bn_16', 'leaky_17', 'conv_17', 'bn_17', 'leaky_18', 'shortcut_18', 'conv_19', 'bn_19', 'leaky_20', 'conv_20', 'bn_20', 'leaky_21', 'shortcut_21', 'conv_22', 'bn_22', 'leaky_23', 'conv_23', 'bn_23', 'leaky_24', 'shortcut_24', 'conv_25', 'bn_25', 'leaky_26', 'conv_26', 'bn_26', 'leaky_27', 'shortcut_27', 'conv_28', 'bn_28', 'leaky_29', 'conv_29', 'bn_29', 'leaky_30', 'shortcut_30', 'conv_31', 'bn_31', 'leaky_32', 'conv_32', 'bn_32', 'leaky_33', 'shortcut_33', 'conv_34', 'bn_34', 'leaky_35', 'conv_35', 'bn_35', 'leaky_36', 'shortcut_36', 'conv_37', 'bn_37', 'leaky_38', 'conv_38', 'bn_38', 'leaky_39', 'conv_39', 'bn_39', 'leaky_40', 'shortcut_40', 'conv_41', 'bn_41', 'leaky_42', 'conv_42', 'bn_42', 'leaky_43', 'shortcut_43', 'conv_44', 'bn_44', 'leaky_45', 'conv_45', 'bn_45', 'leaky_46', 'shortcut_46', 'conv_47', 'bn_47', 'leaky_48', 'conv_48', 'bn_48', 'leaky_49', 'shortcut_49', 'conv_50', 'bn_50', 'leaky_51', 'conv_51', 'bn_51', 'leaky_52', 'shortcut_52', 'conv_53', 'bn_53', 'leaky_54', 'conv_54', 'bn_54', 'leaky_55', 'shortcut_55', 'conv_56', 'bn_56', 'leaky_57', 'conv_57', 'bn_57', 'leaky_58', 'shortcut_58', 'conv_59', 'bn_59', 'leaky_60', 'conv_60', 'bn_60', 'leaky_61', 'shortcut_61', 'conv_62', 'bn_62', 'leaky_63', 'conv_63', 'bn_63', 'leaky_64', 'conv_64', 'bn_64', 'leaky_65', 'shortcut_65', 'conv_66', 'bn_66', 'leaky_67', 'conv_67', 'bn_67', 'leaky_68', 'shortcut_68', 'conv_69', 'bn_69', 'leaky_70', 'conv_70', 'bn_70', 'leaky_71', 'shortcut_71', 'conv_72', 'bn_72', 'leaky_73', 'conv_73', 'bn_73', 'leaky_74', 'shortcut_74', 'conv_75', 'bn_75', 'leaky_76', 'conv_76', 'bn_76', 'leaky_77', 'conv_77', 'bn_77', 'leaky_78', 'conv_78', 'bn_78', 'leaky_79', 'conv_79', 'bn_79', 'leaky_80', 'conv_80', 'bn_80', 'leaky_81', 'conv_81', 'permute_82', 'yolo_82', 'identity_83', 'conv_84', 'bn_84', 'leaky_85', 'upsample_85', 'concat_86', 'conv_87', 'bn_87', 'leaky_88', 'conv_88', 'bn_88', 'leaky_89', 'conv_89', 'bn_89', 'leaky_90', 'conv_90', 'bn_90', 'leaky_91', 'conv_91', 'bn_91', 'leaky_92', 'conv_92', 'bn_92', 'leaky_93', 'conv_93', 'permute_94', 'yolo_94', 'identity_95', 'conv_96', 'bn_96', 'leaky_97', 'upsample_97', 'concat_98', 'conv_99', 'bn_99', 'leaky_100', 'conv_100', 'bn_100', 'leaky_101', 'conv_101', 'bn_101', 'leaky_102', 'conv_102', 'bn_102', 'leaky_103', 'conv_103', 'bn_103', 'leaky_104', 'conv_104', 'bn_104', 'leaky_105', 'conv_105', 'permute_106', 'yolo_106')\n",
      "--- nb couches de sortie du modèle YOLO ---\n",
      "3\n",
      "--- type des couches de sortie du modèle YOLO ---\n",
      "<class 'numpy.ndarray'>\n",
      "--- conversion de l'image ---\n",
      "--- détection d'objets ---\n",
      "--- Analyse des résultats de la détection ---\n",
      "--- suppression non maximale pour éliminer les doublons ---\n",
      "--- Afficher l'image avec les objets détectés et leur étiquette ---\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Charger les fichiers de configuration et de poids du modèle YOLO\n",
    "net = cv2.dnn.readNet(\"yolo/yolov3.weights\", \"yolo/yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "print(\"--- len(layer_names) ---\")\n",
    "print(len(layer_names))\n",
    "\n",
    "print(\"--- type(layer_names) ---\")\n",
    "print(type(layer_names))\n",
    "\n",
    "print(\"--- layer_names ---\")\n",
    "print(layer_names)\n",
    "\n",
    "print(\"--- nb couches de sortie du modèle YOLO ---\")\n",
    "print(len(net.getUnconnectedOutLayers()))\n",
    "\n",
    "print(\"--- type des couches de sortie du modèle YOLO ---\")\n",
    "print(type(net.getUnconnectedOutLayers()))\n",
    "\n",
    "output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Charger les noms des classes (objets que YOLO peut détecter)\n",
    "with open(\"yolo/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Charger l'image à traiter\n",
    "image = cv2.imread('bus-voitures.jpg')  # Remplacez 'image.jpg' par le chemin de votre image\n",
    "height, width, channels = image.shape\n",
    "\n",
    "print(\"--- conversion de l'image ---\")\n",
    "\n",
    "# Convertir l'image en un format adapté pour YOLO (un blob)\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "print(\"--- détection d'objets ---\")\n",
    "\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# Liste pour stocker les sous-images avec leur étiquette et confiance\n",
    "detected_images = []\n",
    "\n",
    "print(\"--- Analyse des résultats de la détection ---\")\n",
    "\n",
    "# Analyser les résultats de la détection\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:  # Seuil de confiance\n",
    "            # Extraire les coordonnées des boîtes englobantes\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            \n",
    "            # Calculer les coordonnées du coin supérieur gauche de la boîte\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "            \n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n",
    "\n",
    "print(\"--- suppression non maximale pour éliminer les doublons ---\")\n",
    "\n",
    "# Appliquer la suppression non maximale pour éliminer les doublons (overlapping boxes)\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "# Afficher les résultats sur l'image\n",
    "for i in range(len(boxes)):\n",
    "    if i in indices:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = str(round(confidences[i], 2))\n",
    "        color = (0, 255, 0)  # Vert\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label + \" \" + confidence, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "print(\"--- Afficher l'image avec les objets détectés et leur étiquette ---\")\n",
    "\n",
    "# Afficher l'image avec les objets détectés\n",
    "cv2.imshow(\"bus-voitures\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cette cellule est un exemple de conversion de coordonnées spatiales en des données normalisées au format yolo.\n",
    "\n",
    "Les coordonnées en entrée sont celles d'une boîte englobante (x_min, y_min, x_max, y_max) typique yolo\n",
    "Il est nécessaire également de connaître la taille de l'image (width_image, height_image)\n",
    "\n",
    "La fonction coord_norm retourne les coordonnées normalisées de YOLO (center_x, center_y, width, height).\n",
    "\n",
    "En entrée le fichier ayant plusieurs coordonnées de rectangles dans l'image, chacu définissant un objet labellisé,\n",
    "En 1ère ligne la taille de l'image\n",
    "Et les lignes suivantes et jusqu'à la fin les coordonnées (x_min, y_min, x_max, y_max) des rectangles (appelés boîtes englobantes dans la sémantique Yolo).\n",
    "\n",
    "A noter que dans une version plus récente de Yolo (après v3), le modèle gère les coordonnées devenant des segments; le rectangle devient un polygone à plusieurs sommets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers: [4080, 3072]\n",
      "row: ['572', '148', '980', '2434']\n",
      "coord:(0.19019607843137254, 0.4202473958333333, 0.1, 0.744140625)\n",
      "row: ['837', '342', '1343', '2310']\n",
      "coord:(0.26715686274509803, 0.431640625, 0.12401960784313726, 0.640625)\n",
      "row: ['1214', '328', '1697', '2434']\n",
      "coord:(0.3567401960784314, 0.4495442708333333, 0.11838235294117647, 0.685546875)\n",
      "row: ['1669', '352', '1948', '2350']\n",
      "coord:(0.4432598039215686, 0.4397786458333333, 0.06838235294117648, 0.650390625)\n",
      "row: ['1945', '345', '2148', '2350']\n",
      "coord:(0.501593137254902, 0.4386393229166667, 0.04975490196078431, 0.6526692708333334)\n",
      "row: ['2091', '430', '2366', '2333']\n",
      "coord:(0.5462009803921568, 0.44970703125, 0.06740196078431372, 0.6194661458333334)\n",
      "row: ['2295', '427', '2675', '2333']\n",
      "coord:(0.6090686274509803, 0.44921875, 0.09313725490196079, 0.6204427083333334)\n",
      "row: ['2559', '593', '2879', '2309']\n",
      "coord:(0.6664215686274509, 0.4723307291666667, 0.0784313725490196, 0.55859375)\n",
      "row: ['2770', '484', '3147', '2326']\n",
      "coord:(0.7251225490196078, 0.4573567708333333, 0.09240196078431373, 0.599609375)\n",
      "row: ['3021', '471', '3395', '2336']\n",
      "coord:(0.7862745098039216, 0.4568684895833333, 0.09166666666666666, 0.6070963541666666)\n",
      "row: ['3242', '546', '3677', '2346']\n",
      "coord:(0.8479166666666667, 0.470703125, 0.10661764705882353, 0.5859375)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def coord_norm(width_image, height_image, x_min, y_min, x_max, y_max):\n",
    "    # Calcul des coordonnées normalisées\n",
    "    center_x = (x_min + x_max) / 2 / width_image\n",
    "    center_y = (y_min + y_max) / 2 / height_image\n",
    "    width = (x_max - x_min) / width_image\n",
    "    height = (y_max - y_min) / height_image\n",
    "    \n",
    "    return center_x, center_y, width, height\n",
    "\n",
    "classe_livre = '0'\n",
    "\n",
    "# Lire le fichier classe csv et écriture dans le fichier classe txt\n",
    "\n",
    "with open(\"images/train/livres.csv\", mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    \n",
    "    # Lire les en-têtes (première ligne)\n",
    "    headers = next(csvreader)\n",
    "    headers=[int(x) for x in headers]  # string transformés en int\n",
    "    \n",
    "    print(f\"headers: {headers}\")\n",
    "    \n",
    "    # Ouvrir un fichier texte pour l'écriture\n",
    "    with open(\"images/train/livres.txt\", mode='w', encoding='utf-8') as txtfile:\n",
    "        \n",
    "        for row in csvreader:\n",
    "            print(f\"row: {row}\")\n",
    "            row=[int(x) for x in row]  # string transformés en int\n",
    "            coord = coord_norm(*headers, *row)\n",
    "            print(f\"coord:{coord}\")\n",
    "            coord=[str(x) for x in coord]  # string transformés en int\n",
    "            txtfile.write(classe_livre + ',' + coord[0] + ',' + coord[1]+ ',' + coord[2]+ ',' + coord[3] + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraînement avec une photo\n",
    "\n",
    "## Dans l'inventaire se trouvent divers objets non préentraînés par yolo.\n",
    "\n",
    "C'est ce que nous allons faire ici par une photo d'une étagère avec des livres.\n",
    "Nous prenons le modèle préentraîné de la version 8 de Yolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.49 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.40  Python-3.12.3 torch-2.5.1+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.yaml, data=data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train25, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train25\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      "YOLOv8s summary: 225 layers, 11,137,148 parameters, 11,137,132 gradients, 28.7 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Admin.local\\Documents\\7-Méthodologie MS\\Projet Machine Learning\\labels\\train.cache... 1 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Admin.local\\Documents\\7-Méthodologie MS\\Projet Machine Learning\\labels\\val.cache... 1 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train25\\labels.jpg... \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Charger l'image à prédire\n",
    "image = cv2.imread(\"images/train/livres.jpg\")\n",
    "\n",
    "# Charger le modèle préexistant (ou choisir un modèle de base)\n",
    "model = YOLO('yolov8s.yaml')  # ou 'yolov8s.yaml' pour YOLOv8\n",
    "\n",
    "# Entraîner le modèle sur vos données\n",
    "model.train(data='data.yaml', epochs=50, batch=16, imgsz=640)\n",
    "\n",
    "# Sauvegarder le modèle après l'entraînement\n",
    "model.save('my_yolov8_model.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La version des différentes librairies Yolo, Torch, Pytorch, Cuda fait que le programme tombe régulièrement en erreur à différents stades du projet. \n",
    "\n",
    "L'erreur rencontrée lors de la sauvegarde du modèle mentionne :\n",
    "torch.save({**self.ckpt, **updates}, filename)\n",
    "TypeError: 'NoneType' object is not a mapping\n",
    "\n",
    "Un contournement possible est de faire appel à \n",
    "torch.save(model.state_dict(), 'my_yolov8s_model.pt')\n",
    "au lieu de model.save('my_yolov8_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Charger l'image à prédire\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# image = cv2.imread(\"images/train/livres.jpg\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Charger le modèle préexistant (ou choisir un modèle de base)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# ou 'yolov8s.yaml' pour YOLOv8\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle sur vos données\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "# # Charger l'image à prédire\n",
    "# image = cv2.imread(\"images/train/livres.jpg\")\n",
    "\n",
    "\n",
    "# Charger le modèle préexistant (ou choisir un modèle de base)\n",
    "model = YOLO('yolov8s.yaml')  # ou 'yolov8s.yaml' pour YOLOv8\n",
    "\n",
    "# Entraîner le modèle sur vos données\n",
    "model.train(data='data.yaml', epochs=50, batch=16, imgsz=640)\n",
    "\n",
    "# Sauvegarde inhabituelle\n",
    "# remplacé par torch.save car le modèle n'ait pas encore généré avec un checkpoint valide au moment de la sauvegarde. \n",
    "torch.save(model.state_dict(), 'my_yolov8s_model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour contourner le problème, la sauvegarde contient uniquement l'état du modèle (sans manipuler self.ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance maintenant une prédiction d'objet.\n",
    "Le cas simple pris est celui de l'examen de la même photo.\n",
    "\n",
    "Si l'on a fait une sauvegarde réussie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger le modèle entraîné\n",
    "model = YOLO('my_yolov8_model.pt')\n",
    "\n",
    "# Charger une image pour la prédiction\n",
    "image = cv2.imread(\"images/train/livres.jpg\")\n",
    "\n",
    "# Effectuer la prédiction\n",
    "results = model(image)\n",
    "\n",
    "# Afficher les résultats\n",
    "results.show()  # Affiche l'image avec les annotations de détection\n",
    "results.save()  # Sauvegarde l'image avec les annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la sauvegarde précédente n'a pas réussi, \n",
    "(rappel : car non génération d'un checkpoint valide au moment de la sauvegarde)\n",
    "il faut charger également le schéma du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Charger le modèle entraîné\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_yolov8_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Recréer le modèle en utilisant la même configuration\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# avec le fichier de configuration pour Yolov8\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "# Charger le modèle entraîné\n",
    "model = YOLO('my_yolov8_model.pt')\n",
    "\n",
    "# Recréer le modèle en utilisant la même configuration\n",
    "model = YOLO('yolov8s.yaml')  # avec le fichier de configuration pour Yolov8\n",
    "model.load_state_dict(torch.load('my_yolov8s_model.pt'))  # Charger les poids sauvegardés par torch.save\n",
    "\n",
    "# Charger une image pour la prédiction\n",
    "image = cv2.imread(\"images/train/livres.jpg\")\n",
    "\n",
    "# Effectuer la prédiction\n",
    "results = model(image)\n",
    "\n",
    "# Afficher les résultats\n",
    "results.show()  # Affiche l'image avec les annotations de détection\n",
    "results.save()  # Sauvegarde l'image avec les annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne pas tenir compte du dernier message d'erreur (NameError: name 'YOLO' is not defined), celui-ci n'est qu'une exécution partielle de la dernière cellule. \n",
    "\n",
    "\n",
    "## Le code devrait s'exécuter.\n",
    "Une mauvaise interaction entre les bibliothèques Yolo, Python (3.13 puis 3.12.7), Pytorch fait lever une exception à l'exécution de la cellule (et des suivantes) sous Jupyter.\n",
    "Par ailleurs le code des dernères cellules marche partiellement (toujours pour la même raison).\n",
    "Le calcul passe ou non selon le paramétrage de Cuda; ici sur W11 je ne réussis pas à lancer le traitement sur la GPU \n",
    "\n",
    "Une amélioration est prévue avec plusieurs axes à suivre:\n",
    "* Augmentation du nombre d'images (train, val)\n",
    "* Modification du paramétrage data.yaml (modèle yolov8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
